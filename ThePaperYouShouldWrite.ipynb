{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize documents and their differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/run/user/1000/jupyter/kernel-57ac752b-6d1a-4975-881f-26db45a2b774.json'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipykernel\n",
    "ipykernel.get_connection_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = \"string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import gensim\n",
    "from loadCorpus import loadModel\n",
    "from random import randint\n",
    "import matplotlib\n",
    "\n",
    "def drawEmbeddingSamples(num=1000, model=None):\n",
    "    if not isinstance(model, gensim.models.doc2vec.Doc2Vec):\n",
    "        model = loadModel(dim=600)\n",
    "    doctags = list(model.docvecs.doctags)\n",
    "    randomSamples = [doctags.pop(randint(0, len(doctags)-(spok + 1))) for spok in range(num)]\n",
    "    return {x: model.docvecs[x] for x in randomSamples}\n",
    "\n",
    "def tsne(docs, metric='euclidean'):\n",
    "    # calulate the t-SNE representation\n",
    "    tsne = TSNE(n_components=2, random_state=0, metric=metric)\n",
    "    return tsne.fit_transform(docs)\n",
    "\n",
    "def randomColors(N=5, candidates=list(matplotlib.colors.cnames.keys())):\n",
    "    return [candidates[randint(0, len(candidates)-1)] for _ in range(N)]\n",
    "\n",
    "def myCosine(X, Y=None, metric='cosine', n_jobs=1, **kwds):\n",
    "    import pdb; pdb.set_trace()\n",
    "    bar = pairwise_distances(X, Y=Y, metric=metric, n_jobs=n_jobs, **kwds)\n",
    "    bar[bar < 0] = 0\n",
    "    return bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and pick the desired documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from loadCorpus import loadModel\n",
    "model = loadModel(dim=600)\n",
    "docvecs = model.docvecs\n",
    "#X,y = loadCorpus(dim=100, model=model)\n",
    "\n",
    "fuest2007 = docvecs['550246665.pdf']\n",
    "fuest1999 =  docvecs['cesifo_wp215.pdf']\n",
    "sinn2007 = docvecs['555918033.PDF']\n",
    "sinn2000 = docvecs['cesifo_wp307.pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuest's difference +/- Sinn's documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "numSamples = 5000\n",
    "\n",
    "fuestDiff = fuest1999 - fuest2007\n",
    "# snm -> Sinn, New, Minus fuestDiff\n",
    "# som -> Sinn, Old, Minus fuestDiff\n",
    "# ...\n",
    "snm = sinn2007 - fuestDiff\n",
    "som = sinn2000 - fuestDiff\n",
    "snp = sinn2007 + fuestDiff\n",
    "sop = sinn2000 + fuestDiff\n",
    "\n",
    "docVecsSmF = [snm, som, snp, sop, fuestDiff, fuest2007, fuest1999, \\\n",
    "                 sinn2007, sinn2000]\n",
    "docVecsSmFMapping = ['snm', 'som', 'snp', 'sop', 'fuest2007 - fuest1999', \\\n",
    "                        'fuest2007', 'fuest1999', 'sinn2007', 'sinn2000']\n",
    "docVecsSmFColors = ['red'] * 4 + ['yellow', \n",
    "                                     '#00ccff', # light blue\n",
    "                                     '#0000cc', # blue\n",
    "                                     '#66ff33', # light green\n",
    "                                     '#39ac73'] # green\n",
    "\n",
    "randomSamples = list(drawEmbeddingSamples(numSamples, model=model).values())\n",
    "\n",
    "docs = docVecsSmF + randomSamples\n",
    "docColors = docVecsSmFColors + ['black'] * numSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsneDocs = tsne(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsneIfo = tsneDocs[:len(docVecsSmF), :]\n",
    "tsneSamples = tsneDocs[len(docVecsSmF):, :]\n",
    "\n",
    "handles = []\n",
    "plt.figure(figsize=(15, 12), dpi=100)\n",
    "for docVecs, c in zip(tsneIfo, docColors):\n",
    "    handles.append(plt.scatter(docVecs[0], docVecs[1], s=250,\\\n",
    "                   c=c, alpha=.5, cmap=plt.cm.Spectral))\n",
    "\n",
    "handles.append(plt.scatter(tsneSamples[:, 0], tsneSamples[:, 1], s=50, c='black',\\\n",
    "                           alpha=.05, cmap=plt.cm.Spectral))\n",
    "plt.title(\"t-SNE\")\n",
    "plt.legend(handles, docVecsSmFMapping)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinn's difference +/- Fuest's documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sinnDiff = sinn2000 - sinn2007\n",
    "fnm = fuest2007 - sinnDiff\n",
    "fom = fuest1999 - sinnDiff\n",
    "fnp = fuest2007 + sinnDiff\n",
    "fop = fuest1999 + sinnDiff\n",
    "\n",
    "docVecsFmS = [fnm, fom, fnp, fop, fuestDiff, fuest2007, fuest1999, \\\n",
    "                sinn2007, sinn2000]\n",
    "docVecsFmSMapping = ['fnm', 'fom', 'fnp', 'fop', 'sinn2000 - sinn2007', \\\n",
    "                        'fuest2007', 'fuest1999', 'sinn2007', 'sinn2000']\n",
    "docVecsFmSColors = ['red'] * 4 + ['yellow', \n",
    "                                     '#00ccff', # light blue\n",
    "                                     '#0000cc', # blue\n",
    "                                     '#66ff33', # light green\n",
    "                                     '#39ac73'] # green\n",
    "\n",
    "randomSamples = list(drawEmbeddingSamples(numSamples, model=model).values())\n",
    "\n",
    "docs = docVecsFmS + randomSamples\n",
    "docColors = docVecsFmSColors + ['black'] * numSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsneDocs = tsne(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsneIfo = tsneDocs[:len(docVecsFmS), :]\n",
    "tsneSamples = tsneDocs[len(docVecsFmS):, :]\n",
    "\n",
    "handles = []\n",
    "plt.figure(figsize=(15, 12), dpi=100)\n",
    "for docVecs, c in zip(tsneIfo, docColors):\n",
    "    handles.append(plt.scatter(docVecs[0], docVecs[1], s=250,\\\n",
    "                   c=c, alpha=.5, cmap=plt.cm.Spectral))\n",
    "\n",
    "handles.append(plt.scatter(tsneSamples[:, 0], tsneSamples[:, 1], s=50, c='black',\\\n",
    "                           alpha=.05, cmap=plt.cm.Spectral))\n",
    "plt.title(\"t-SNE\")\n",
    "plt.legend(handles, docVecsFmSMapping)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE by authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topUSAuthors = {\n",
    "    'James J. Heckman': {\n",
    "        'color': 'red',\n",
    "        'tsne': [],\n",
    "        'docs': [\"791344649.pdf.json\",\n",
    "            \"wp03-04.pdf.json\",\n",
    "            \"664338003.pdf.json\",\n",
    "            \"dp8827.pdf.json\",\n",
    "            \"728401312.pdf.json\",\n",
    "            \"dp8027.pdf.json\",\n",
    "            \"cesifo_wp1031.pdf.json\",\n",
    "            \"dp8711.pdf.json\",\n",
    "            \"dp7750.pdf.json\",\n",
    "            \"732416388.pdf.json\",\n",
    "            \"558854818.pdf.json\",\n",
    "            \"wp03-13.pdf.json\",\n",
    "            \"632184426.pdf.json\",\n",
    "            \"632185252.pdf.json\",\n",
    "            \"490361323.pdf.json\",\n",
    "            \"cesifo_wp1014.pdf.json\",\n",
    "            \"dp7550.pdf.json\",\n",
    "            \"732553164.pdf.json\",\n",
    "            \"66523810X.pdf.json\",\n",
    "            \"362526656.pdf.json\",\n",
    "            \"374816565.pdf.json\",\n",
    "            \"dp7628.pdf.json\",\n",
    "            \"490471781.pdf.json\",\n",
    "            \"687928893.pdf.json\",\n",
    "            \"484623192.pdf.json\",\n",
    "            \"801006074.pdf.json\",\n",
    "            \"717449475.pdf.json\",\n",
    "            \"dp8200.pdf.json\",\n",
    "            \"675944805.pdf.json\",\n",
    "            \"dp9476.pdf.json\",\n",
    "            \"wp02-02.pdf.json\",\n",
    "            \"wp03-09.pdf.json\",\n",
    "            \"801004721.pdf.json\",\n",
    "            \"715979043.pdf.json\",\n",
    "            \"665267223.pdf.json\",\n",
    "            \"749691735.pdf.json\",\n",
    "            \"632183454.pdf.json\",\n",
    "            \"idb-wp_430.pdf.json\",\n",
    "            \"716017407.pdf.json\",\n",
    "            \"dp7552.pdf.json\",\n",
    "            \"wp03-17.pdf.json\",\n",
    "            \"pp17.pdf.json\",\n",
    "            \"dp9247.pdf.json\",\n",
    "            \"558856314.pdf.json\",\n",
    "            \"727557017.pdf.json\",\n",
    "            \"dp8548.pdf.json\",\n",
    "            \"757433537.pdf.json\",\n",
    "            \"dp8424.pdf.json\",\n",
    "            \"659505134.pdf.json\",\n",
    "            \"dp8338.pdf.json\",\n",
    "            \"dp8696.pdf.json\",\n",
    "            \"664348866.pdf.json\",\n",
    "            \"663112346.pdf.json\",\n",
    "            \"wp1408.pdf.json\",\n",
    "            \"607538317.pdf.json\",\n",
    "            \"362626855.pdf.json\",\n",
    "            \"638495555.pdf.json\"]\n",
    "    },\n",
    "    'Barry Julian Eichengreen': {\n",
    "        'color': 'fuchsia',\n",
    "        'tsne': [],\n",
    "        'docs': [\"730571912.pdf.json\",\n",
    "            \"wp-004.pdf.json\",\n",
    "            \"idb-wp_558.pdf.json\",\n",
    "            \"360795722.pdf.json\",\n",
    "            \"510276172.pdf.json\",\n",
    "            \"796867208.pdf.json\",\n",
    "            \"77656370X.pdf.json\",\n",
    "            \"ewp-262.pdf.json\",\n",
    "            \"656407387.pdf.json\",\n",
    "            \"590225650.PDF.json\",\n",
    "            \"61801697X.pdf.json\",\n",
    "            \"642338310.pdf.json\",\n",
    "            \"729180689.pdf.json\"],\n",
    "    },\n",
    "    'Daron Acemoglu': {\n",
    "        'color': 'gold',\n",
    "        'tsne': [],\n",
    "        'docs': [\"477687644.pdf.json\",\n",
    "            \"686844475.pdf.json\",\n",
    "            \"833124862.pdf.json\",\n",
    "            \"dp9068.pdf.json\",\n",
    "            \"612963969.pdf.json\",\n",
    "            \"567037134.pdf.json\",\n",
    "            \"dp7906.pdf.json\",\n",
    "            \"cesifo_wp5366.pdf.json\",\n",
    "            \"VfS_2010_pid_558.pdf.json\"]\n",
    "    },\n",
    "    'Joseph E. Stiglitz': {\n",
    "        'color': 'sienna',\n",
    "        'tsne': [],\n",
    "        'docs': [\"640462014.pdf.json\",\n",
    "            \"576782793.pdf.json\",\n",
    "            \"309202949.pdf.json\",\n",
    "            \"51214043X.pdf.json\",\n",
    "            \"771928769.pdf.json\",\n",
    "            \"826742238.pdf.json\"]\n",
    "    },\n",
    "    'Christopher F Baum': {\n",
    "        'color': 'blue',\n",
    "        'tsne': [],\n",
    "        'docs': [\"dp633.pdf.json\",\n",
    "            \"dp634.pdf.json\",\n",
    "            \"dp638.pdf.json\",\n",
    "            \"dp410.pdf.json\",\n",
    "            \"623004666.pdf.json\",\n",
    "            \"772388652.pdf.json\",\n",
    "            \"dp635.pdf.json\",\n",
    "            \"dp443.pdf.json\",\n",
    "            \"dp0410.pdf.json\",\n",
    "            \"diw_finess_03030.pdf.json\",\n",
    "            \"595251412.PDF.json\",\n",
    "            \"606801979.pdf.json\"]\n",
    "    },\n",
    "    'Carmen M. Reinhart': {\n",
    "        'color': 'darkgoldenrod',\n",
    "        'tsne': [],\n",
    "        'docs': [\"cesifo_wp5422.pdf.json\",\n",
    "            \"idb-wp_457.pdf.json\",\n",
    "            \"idb-wp_458.pdf.json\",\n",
    "            \"732720230.pdf.json\",\n",
    "            \"lmu-mdp_2014-49.pdf.json\",\n",
    "            \"idb-wp_302.pdf.json\",\n",
    "            \"687820979.pdf.json\"]\n",
    "    },\n",
    "    'Thomas J. Sargent': {\n",
    "        'color': 'white',\n",
    "        'tsne': [],\n",
    "        'docs': [\"wp2003-14.pdf.json\",\n",
    "            \"200528dkp.pdf.json\",\n",
    "            \"wp2003-25.pdf.json\",\n",
    "            \"383913152.PDF.json\",\n",
    "            \"505119463.pdf.json\",\n",
    "            \"wp481.pdf.json\",\n",
    "            \"505119412.pdf.json\",\n",
    "            \"wp2005-09.pdf.json\",\n",
    "            \"82835975X.pdf.json\",\n",
    "            \"wp2004-22.pdf.json\",\n",
    "            \"591928027.pdf.json\",\n",
    "            \"572292899.pdf.json\"]\n",
    "    },\n",
    "    'M Hashem Pesaran': {\n",
    "        'color': 'green',\n",
    "        'tsne': [],\n",
    "        'docs': [\"736674640.pdf.json\",\n",
    "            \"cesifo1_wp1599.pdf.json\",\n",
    "            \"734622074.pdf.json\",\n",
    "            \"cesifo_wp5428.pdf.json\",\n",
    "            \"660761904.pdf.json\",\n",
    "            \"666546231.pdf.json\",\n",
    "            \"715801236.pdf.json\",\n",
    "            \"59283526X.PDF.json\",\n",
    "            \"cesifo_wp4508.pdf.json\",\n",
    "            \"cesifo_wp995.pdf.json\",\n",
    "            \"559866755.pdf.json\",\n",
    "            \"604523742.pdf.json\",\n",
    "            \"559459076.pdf.json\",\n",
    "            \"555510999.pdf.json\",\n",
    "            \"cesifo_wp4834.pdf.json\",\n",
    "            \"720705274.pdf.json\",\n",
    "            \"515328278.PDF.json\",\n",
    "            \"538177373.PDF.json\",\n",
    "            \"66205587X.pdf.json\",\n",
    "            \"517049996.PDF.json\",\n",
    "            \"568422740.PDF.json\",\n",
    "            \"200627dkp.pdf.json\",\n",
    "            \"cesifo_wp4371.pdf.json\",\n",
    "            \"720581133.pdf.json\",\n",
    "            \"cesifo_wp5434.pdf.json\",\n",
    "            \"cesifo_wp990.pdf.json\",\n",
    "            \"dp1196.pdf.json\",\n",
    "            \"cesifo1_wp1425.pdf.json\",\n",
    "            \"727117904.pdf.json\",\n",
    "            \"548147493.pdf.json\",\n",
    "            \"564825875.PDF.json\",\n",
    "            \"cesifo_wp869.pdf.json\",\n",
    "            \"IDB-WP-510.pdf.json\",\n",
    "            \"551460059.pdf.json\",\n",
    "            \"68520281X.pdf.json\",\n",
    "            \"cesifo1_wp1650.pdf.json\",\n",
    "            \"538347554.PDF.json\",\n",
    "            \"715937049.pdf.json\",\n",
    "            \"cesifo1_wp1477.pdf.json\",\n",
    "            \"51578544X.pdf.json\",\n",
    "            \"715931490.pdf.json\",\n",
    "            \"555968669.PDF.json\",\n",
    "            \"cesifo_wp4592.pdf.json\",\n",
    "            \"544121406.pdf.json\",\n",
    "            \"55799005X.PDF.json\",\n",
    "            \"717914593.pdf.json\",\n",
    "            \"627338070.pdf.json\",\n",
    "            \"559866593.pdf.json\",\n",
    "            \"cesifo1_wp1416.pdf.json\",\n",
    "            \"economics_2007-3.pdf.json\",\n",
    "            \"dp1108.pdf.json\",\n",
    "            \"617482330.pdf.json\",\n",
    "            \"665579225.pdf.json\",\n",
    "            \"551074620.pdf.json\",\n",
    "            \"532017501.pdf.json\",\n",
    "            \"715366505.pdf.json\",\n",
    "            \"619071087.pdf.json\",\n",
    "            \"cesifo1_wp1233.pdf.json\",\n",
    "            \"570161258.PDF.json\",\n",
    "            \"732540674.pdf.json\",\n",
    "            \"dp1236.pdf.json\",\n",
    "            \"cesifo1_wp1438.pdf.json\",\n",
    "            \"626619505.pdf.json\",\n",
    "            \"200542dkp.pdf.json\",\n",
    "            \"517025035.PDF.json\",\n",
    "            \"cesifo1_wp1548.pdf.json\",\n",
    "            \"538034203.PDF.json\",\n",
    "            \"cesifo1_wp1565.pdf.json\",\n",
    "            \"cesifo1_wp1237.pdf.json\",\n",
    "            \"cesifo_wp1176.pdf.json\",\n",
    "            \"661602680.pdf.json\",\n",
    "            \"514746963.pdf.json\",\n",
    "            \"560540418.PDF.json\",\n",
    "            \"529380676.PDF.json\",\n",
    "            \"cesifo1_wp1531.pdf.json\",\n",
    "            \"715726412.pdf.json\",\n",
    "            \"cesifo_wp5410.pdf.json\",\n",
    "            \"615085288.pdf.json\",\n",
    "            \"cesifo1_wp1358.pdf.json\",\n",
    "            \"71745228X.pdf.json\",\n",
    "            \"cesifo1_wp1659.pdf.json\",\n",
    "            \"cesifo_wp4433.pdf.json\",\n",
    "            \"518893278.PDF.json\",\n",
    "            \"cesifo1_wp1331.pdf.json\",\n",
    "            \"52873816X.PDF.json\",\n",
    "            \"cesifo_wp4983.pdf.json\",\n",
    "            \"559087853.PDF.json\",\n",
    "            \"538378069.pdf.json\",\n",
    "            \"cesifo_wp4871.pdf.json\",\n",
    "            \"587537787.pdf.json\",\n",
    "            \"cesifo_wp4736.pdf.json\",\n",
    "            \"cesifo_wp4822.pdf.json\",\n",
    "            \"612935043.pdf.json\",\n",
    "            \"cesifo_wp5367.pdf.json\",\n",
    "            \"631014306.pdf.json\",\n",
    "            \"559090684.PDF.json\",\n",
    "            \"62965543X.pdf.json\",\n",
    "            \"cesifo_wp1169.pdf.json\",\n",
    "            \"cesifo_wp4232.pdf.json\",\n",
    "            \"kap1366.pdf.json\",\n",
    "            \"cesifo1_wp1229.pdf.json\",\n",
    "            \"516953184.PDF.json\",\n",
    "            \"dp1240.pdf.json\",\n",
    "            \"690002343.pdf.json\",\n",
    "            \"631381295.pdf.json\",\n",
    "            \"685277747.pdf.json\",\n",
    "            \"557255945.PDF.json\",\n",
    "            \"615344860.pdf.json\",\n",
    "            \"538294833.PDF.json\",\n",
    "            \"cesifo_wp374.pdf.json\",\n",
    "            \"548147248.pdf.json\",\n",
    "            \"cesifo_wp4807.pdf.json\",\n",
    "            \"659395266.pdf.json\",\n",
    "            \"dp1313.pdf.json\",\n",
    "            \"669983144.pdf.json\",\n",
    "            \"669863831.pdf.json\",\n",
    "            \"dp2007-7.pdf.json\",\n",
    "            \"cesifo1_wp1308.pdf.json\",\n",
    "            \"558342329.PDF.json\"]\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from loadCorpus import loadModel\n",
    "\n",
    "try:\n",
    "    if not isinstance(model, gensim.models.doc2vec.Doc2Vec):\n",
    "        model = loadModel(dim=600)\n",
    "except NameError:\n",
    "    model = loadModel(dim=600)\n",
    "\n",
    "corpus = list(drawEmbeddingSamples(num=9000, model=model).values())\n",
    "\n",
    "# flatten the datastructure to obtain all documents' file names\n",
    "topUSAuthorsDocs = itertools.chain(*(author['docs'] \\\n",
    "                                     for author in list(topUSAuthors.values())))\n",
    "topUSAuthorsDocs = list(map(lambda x: x.replace(\".json\", \"\"), topUSAuthorsDocs))\n",
    "topUSAuthorsDocEmbeddings = [docvecs[doc] for doc in topUSAuthorsDocs]\n",
    "\n",
    "# merging samples and the selected docs\n",
    "corpus = list(itertools.chain(topUSAuthorsDocEmbeddings, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsneCorpus = tsne(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the result\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tsneUSAuthors = tsneCorpus[:len(topUSAuthorsDocs), :]\n",
    "tsneSamples = tsneCorpus[len(topUSAuthorsDocs):, :]\n",
    "preferedColors = ['green', 'mediumorchid', 'darkgoldenrod', 'blue', \\\n",
    "                  'sienna', 'gold', 'fuchsia', 'red', 'white']\n",
    "# asign color to author and\n",
    "# asign tsne representations to documents\n",
    "docsProcessed = 0\n",
    "for author, values in topUSAuthors.items():\n",
    "    #values['color'] = randomColors(N=1, candidates=preferedColors)\n",
    "    \n",
    "    numDocs = len(values['docs'])\n",
    "    values['tsne'] = tsneUSAuthors[docsProcessed:(docsProcessed+numDocs)]\n",
    "    docsProcessed += numDocs\n",
    "    \n",
    "handles = []\n",
    "plt.figure(figsize=(20, 20), dpi=100)\n",
    "for author, values in topUSAuthors.items():\n",
    "    handles.append(plt.scatter(values['tsne'][:, 0], values['tsne'][:, 1], s=250,\\\n",
    "                   c=values['color'], alpha=.5, cmap=plt.cm.Spectral))\n",
    "\n",
    "handles.append(plt.scatter(tsneSamples[:, 0], tsneSamples[:, 1], s=50, c='black',\\\n",
    "                           alpha=.05, cmap=plt.cm.Spectral))\n",
    "plt.title(\"t-SNE\")\n",
    "plt.legend(handles, [author for author in topUSAuthors.keys()])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "## cluster @-8,.5\n",
    "### statistic models\n",
    "M Hashem Pesaran -> cesifo1_wp1599.pdf -> models of expectations formation, survey data, heterogeneity, tests of rational expectations\n",
    "M Hashem Pesaran -> 734622074.pdf -> dynamic discrete choice, fixed effects, panel data, initial values, GMM, CMLE\n",
    "James J. Heckman -> 632185252.pdf -> Correlated random coefficient, testing, instrumental variables, power of\n",
    "tests based on IV\n",
    "\n",
    "## cluster @0,3\n",
    "### risk management/natural resources/oil and gas\n",
    "M Hashem Pesaran -> cesifo_wp995.pdf -> risk management, economic interlinkages, loss forecasting, default correlation \n",
    "M Hashem Pesaran -> dp2007-7.pdf -> Global VAR, interdependencies, Fisher relationship, Uncovered Interest Rate Parity, Purchasing Power Parity, persistence profile, error variance decomposition\n",
    "M Hashem Pesaran -> 690002343.pdf -> growth models, long run and error correcting relations, major oil exporters, OPEC member countries, oil exports and foreign output shocks.\n",
    "\n",
    "## cluster @6.5,-4\n",
    "### financial risk/ currencies\n",
    "Barry Julian Eichengreen -> 729180689.pdf -> Tackling systemic financial risk\n",
    "Barry Julian Eichengreen -> 642338310.pdf -> The Federal Reserve, the Bank of England, and the Rise of the Dollar as an International Currency, 1914-1939"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Doc2Vec arithmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indicesToDoctags(indices, model=None):\n",
    "    if model == None:\n",
    "        raise TypeError('A model is required')\n",
    "    return {i: model.docvecs.index_to_doctag(i) for i in list(indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    model = loadModel(dim=600)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=10)\n",
    "nbrs.fit(list(docvecs))\n",
    "    \n",
    "X = docvecs['68320193X.pdf']\n",
    "Y = docvecs['525031359.pdf']\n",
    "\n",
    "Z1 = X - Y\n",
    "Z2 = X + Y\n",
    "\n",
    "disZ1, indZ1 = nbrs.kneighbors(Z1.reshape(1,-1))\n",
    "disZ2, indZ2 = nbrs.kneighbors(Z2.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = set(list(indZ1[0]))\n",
    "bar = set(list(indZ2[0]))\n",
    "\n",
    "list(indicesToDoctags(indZ1.flat, model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pick random docs and compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "sampleSize = 1000\n",
    "\n",
    "randRange = [randrange(0, len(docvecs)) for _ in range(sampleSize)]\n",
    "randDocs = docvecs[randRange]\n",
    "distances = [[cosine(i,j) for i in randDocs] for j in randDocs]\n",
    "distances = np.array(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find two close and one distant doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# far distance docs\n",
    "X, A = np.unravel_index(distances.argmax(), distances.shape)\n",
    "\n",
    "# nearby docs\n",
    "nbrs = NearestNeighbors(n_neighbors=2)\n",
    "nbrs.fit(list(randDocs))\n",
    "_, indices = nbrs.kneighbors(randDocs[X].reshape(1,-1))\n",
    "Y = indices[0][1]\n",
    "\n",
    "\n",
    "X = (randRange[X], docvecs.index_to_doctag(randRange[X]))\n",
    "Y = (randRange[Y], docvecs.index_to_doctag(randRange[Y]))\n",
    "A = (randRange[A], docvecs.index_to_doctag(randRange[A]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain difference between X and Y using wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def getSimilarities(a, model=None):\n",
    "    if model == None:\n",
    "        raise TypeError('You need to pass a model')\n",
    "    \n",
    "    # TODO: Can I get rid of the for loop?\n",
    "    for word in model.vocab.keys():\n",
    "        yield word, cosine_similarity(a.reshape(1,-1), model[word].reshape(1,-1)).flatten()[0]\n",
    "\n",
    "        \n",
    "def findClosestWordVec(X, Y, model=None):\n",
    "    '''\n",
    "    Find the wordvec wv that minimizes distance(X+wv, Y)\n",
    "    '''\n",
    "    if model == None:\n",
    "        raise TypeError('You need to pass a model')\n",
    "\n",
    "    diff = Y - X\n",
    "    sims = []\n",
    "    for sim in tqdm(getSimilarities(diff, model=model)):\n",
    "        sims.append(sim)\n",
    "    sims = sorted(sims, key=lambda x: -x[1])\n",
    "    return sims        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "X = docvecs[X[0]]\n",
    "Y = docvecs[Y[0]]\n",
    "newX = X\n",
    "path = [] # keeps track of the vectors that make up the difference\n",
    "\n",
    "logging.info('initial distance is: {}'.format(cosine(X, Y)))\n",
    "for i in tqdm(range(20)):\n",
    "    candidates = findClosestWordVec(newX, Y, model=model)\n",
    "    \n",
    "    # is the performance better now?\n",
    "    # if not, check the next candidate\n",
    "    secondChances = 10\n",
    "    newWord = \"\"\n",
    "    for j in range(0, secondChances):\n",
    "        newWord = model[candidates[j][0]]\n",
    "        if cosine(newX + newWord, Y) < cosine(newX, Y):\n",
    "            logging.info('better performance with: {}'.format(candidates[j][0]))\n",
    "            newWord = model[candidates[j][0]]\n",
    "            break\n",
    "        else:\n",
    "            logging.info('worse performance with: {}'.format(candidates[j][0]))\n",
    "    else:\n",
    "        logging.info('Optimization stopped after {} attempts'.format(secondChances))\n",
    "        break\n",
    "    \n",
    "    \n",
    "    logging.info('{} best word: {}'.format(i, candidates[j][0]))\n",
    "    \n",
    "    path.append(newWord)\n",
    "    newX = newX + newWord\n",
    "    logging.info('cosine distance: {}'.format(cosine(newX, Y)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# artificial example/manual algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = docvecs['VfS_2010_pid_1003.pdf']\n",
    "A = docvecs['02027.pdf']\n",
    "diff = A - X\n",
    "\n",
    "grok = tsne(np.array([X,A,diff]), metric=myCosine)\n",
    "grok = grok * 10000\n",
    "\n",
    "plt.scatter(grok[:, 0], grok[:, 1], s=50, c=docColors, alpha=.7, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getColorRange(d=4):\n",
    "    '''\n",
    "    return d colors between black and white\n",
    "    '''\n",
    "    return ['#000000'] + ['#' + hex(int(floor(i*255./d))).split('x')[1] * 3 for i in range(1, d+1)]\n",
    "\n",
    "X = docvecs['VfS_2010_pid_1003.pdf']\n",
    "A = docvecs['02027.pdf']\n",
    "\n",
    "diff = []\n",
    "diff.append(A - X)\n",
    "diff.append(diff[-1] - model['jobcareer'])\n",
    "diff.append(diff[-1] - model['thatcarbon'])\n",
    "diff.append(diff[-1] + model['empire'])\n",
    "diff.append(diff[-1] + model['goldsmithpinkhamand'])\n",
    "diff.append(diff[-1] - model['countryimmigrant'])\n",
    "diff.append(diff[-1] + model['interestsi'])\n",
    "\n",
    "grok = tsne([X,A,*diff], metric='euclidean')\n",
    "grok = grok * 10000\n",
    "docColors = ['red', 'green'] + list(reversed(getColorRange(len(diff)-1)))\n",
    "\n",
    "plt.scatter(grok[:, 0], grok[:, 1], s=500, c=docColors, alpha=1, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Wikipedia Corpus\n",
    "\n",
    "Inspired by [this](https://radimrehurek.com/gensim/wiki.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim\n",
    "import bz2\n",
    "from os.path import join\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading corpus from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_dir = 'data/wiki_corpus'\n",
    "# load id->word mapping (the dictionary)\n",
    "id2word = gensim.corpora.Dictionary.load_from_text(\n",
    "    bz2.BZ2File(join(corpus_dir, '._wordids.txt.bz2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.corpora import WikiCorpus\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from os.path import isfile\n",
    "import random\n",
    "\n",
    "class Document_Generator:\n",
    "    \"\"\"\n",
    "    Reads a wikipedia dump and generates TaggedDocuments as used\n",
    "    by gensim.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus_file=None, cache_file=None, \\\n",
    "                 p=1, dictionary=None):\n",
    "        \"\"\"\n",
    "        Reads the wikipedia dump passed bycorpus_file, transforms each \n",
    "        article to a json entry in cache_file, using the dictionary \n",
    "        provided in dictionary. p limits the max number of documents\n",
    "        being processed. Document are randomly selected with the \n",
    "        probability p.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            assert corpus_file != None and \\\n",
    "                   cache_file != None and \\\n",
    "                   dictionary != None\n",
    "        except AssertionError:\n",
    "            raise AttributeError('corpus_file, cache_file and ' +\n",
    "                                 'dictionary must be provided')\n",
    "        \n",
    "        self.p = p\n",
    "        self.dictionary = dictionary\n",
    "        self.cache_file = cache_file\n",
    "        wiki_corpus = WikiCorpus(corpus_file, lemmatize=False, \\\n",
    "                                 dictionary=dictionary)\n",
    "        wiki_corpus.metadata = True # to get the articles name and ID\n",
    "\n",
    "        # if file doestn't exists, initialize\n",
    "        if not isfile(cache_file):\n",
    "            with open(cache_file, 'a') as fh:\n",
    "                for article in tqdm(wiki_corpus.get_texts()):\n",
    "                    entry = {}\n",
    "                    text = ' '.join((a.decode('utf8') \\\n",
    "                                     for a in article[0]))\n",
    "                    ID = article[1][0]\n",
    "                    name = article[1][1]\n",
    "                    entry[name] = { 'id': ID, 'text': text}\n",
    "                    fh.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Yields TaggedDocuments using a generator that yields \n",
    "        (<ID>, <Text>) tuples.\n",
    "        \"\"\"\n",
    "        with open(self.cache_file, 'r') as fh:\n",
    "            for i, line in enumerate(fh):\n",
    "                entry = json.loads(line)\n",
    "                if self._true_false_gen():\n",
    "                    for k in entry.keys():\n",
    "                        words = entry[k]['text'].split()\n",
    "                        yield TaggedDocument(words=words, \\\n",
    "                                             tags=[k])\n",
    "                    \n",
    "        raise StopIteration\n",
    "        \n",
    "    def _true_false_gen(self, p = .5):\n",
    "        \"\"\"\n",
    "        Generates True or False with the probability p and 1-p,\n",
    "        respectively\n",
    "        \"\"\"\n",
    "        return random.random() < self.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# write subset of the corpus to disk\n",
    "cache = join(corpus_dir, 'raw', 'all_articles.json')\n",
    "cache_subset = join(corpus_dir, 'raw', 'all_articles_subset_0.1.json')\n",
    "\n",
    "def true_false_gen(p = .5):\n",
    "    return random.random() < p\n",
    "\n",
    "with open(cache, 'r') as fh_r, open(cache_subset, 'w+') as fh_w:\n",
    "    for i, line in tqdm(enumerate(fh_r)):\n",
    "        if true_false_gen(p=0.1):\n",
    "            fh_w.writelines(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import os\n",
    "\n",
    "def load_wikipedia_model(path=None):\n",
    "    if os.path.exists(path):\n",
    "        return Doc2Vec.load(path)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start building vocabulary\n",
      "INFO:gensim.models.doc2vec:collecting all words and their counts\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #10000, processed 16649919 words (3150941/s), 306061 word types, 10000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #20000, processed 29761415 words (3138122/s), 472884 word types, 20000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #30000, processed 40081111 words (3055727/s), 583788 word types, 30000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #40000, processed 48635635 words (3006482/s), 670771 word types, 40000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #50000, processed 56442760 words (2978155/s), 749942 word types, 50000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #60000, processed 63949256 words (3021977/s), 820641 word types, 60000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #70000, processed 70447307 words (2973168/s), 885455 word types, 70000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #80000, processed 76663114 words (3006207/s), 942746 word types, 80000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #90000, processed 82812856 words (3041470/s), 996747 word types, 90000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #100000, processed 88530027 words (2986400/s), 1048587 word types, 100000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #110000, processed 93902262 words (2980306/s), 1099719 word types, 110000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #120000, processed 98962968 words (2770302/s), 1144950 word types, 120000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #130000, processed 103933057 words (2933447/s), 1188946 word types, 130000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #140000, processed 108954409 words (2951173/s), 1234584 word types, 140000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #150000, processed 113980884 words (2845307/s), 1281242 word types, 150000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #160000, processed 118449691 words (2822916/s), 1324338 word types, 160000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #170000, processed 122712520 words (2833033/s), 1360254 word types, 170000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #180000, processed 127078133 words (2735842/s), 1399779 word types, 180000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #190000, processed 131363510 words (2833483/s), 1438679 word types, 190000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #200000, processed 135797522 words (2822299/s), 1472928 word types, 200000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #210000, processed 140396015 words (2637549/s), 1512815 word types, 210000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #220000, processed 144951152 words (2863430/s), 1548433 word types, 220000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #230000, processed 149077465 words (2827536/s), 1583831 word types, 230000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #240000, processed 153385494 words (2829282/s), 1621104 word types, 240000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #250000, processed 157639808 words (2828265/s), 1658647 word types, 250000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #260000, processed 162026313 words (2817068/s), 1693666 word types, 260000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #270000, processed 166263721 words (2855653/s), 1729945 word types, 270000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #280000, processed 170381677 words (2952444/s), 1769001 word types, 280000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #290000, processed 174875615 words (2921210/s), 1805059 word types, 290000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #300000, processed 179147537 words (2547019/s), 1850589 word types, 300000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #310000, processed 183313929 words (2794467/s), 1883953 word types, 310000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #320000, processed 187569217 words (2835500/s), 1917981 word types, 320000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #330000, processed 191600034 words (2870867/s), 1947895 word types, 330000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #340000, processed 195939330 words (2711039/s), 1981085 word types, 340000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #350000, processed 200129618 words (2892573/s), 2015676 word types, 350000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #360000, processed 204156115 words (3037231/s), 2045634 word types, 360000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #370000, processed 208071451 words (3012008/s), 2080940 word types, 370000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #380000, processed 212052568 words (3018053/s), 2116582 word types, 380000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #390000, processed 215689421 words (2992300/s), 2145582 word types, 390000 tags\n",
      "INFO:gensim.models.doc2vec:PROGRESS: at example #400000, processed 219408638 words (3004793/s), 2175066 word types, 400000 tags\n",
      "INFO:gensim.models.doc2vec:collected 2202453 word types and 409372 unique tags from a corpus of 409372 examples and 222483238 words\n",
      "INFO:gensim.models.word2vec:min_count=3 retains 762601 unique words (drops 1439852)\n",
      "INFO:gensim.models.word2vec:min_count leaves 220737897 word corpus (99% of original 222483238)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 2202453 items\n",
      "INFO:gensim.models.word2vec:sample=0 downsamples 0 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 220737897 word corpus (100.0% of prior 220737897)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 762601 words and 1000 dimensions: 8353991100 bytes\n",
      "INFO:gensim.models.word2vec:constructing a huffman tree from 762601 words\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from os.path import join, exists\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "dim = 1000\n",
    "corpus_dir = 'data/wiki_corpus/'\n",
    "model_file_name = '{}_dim_wiki_doc2vec.model'.format(dim)\n",
    "model_path = join(corpus_dir, model_file_name)\n",
    "corpus = join(corpus_dir, 'raw', \\\n",
    "              'enwiki-20160701-pages-articles-multistream.xml.bz2')\n",
    "# cache = join(corpus_dir, 'raw', 'all_articles.json')\n",
    "cache = join(corpus_dir, 'raw', 'all_articles_subset_0.1.json')\n",
    "model = load_wikipedia_model(model_path)\n",
    "\n",
    "if model == None:\n",
    "    model = Doc2Vec(size=dim, window=10, min_count=3, workers=4, \\\n",
    "                            alpha=0.025, min_alpha=0.025)\n",
    "\n",
    "    doc_iter = Document_Generator(corpus_file=corpus, cache_file=cache, \\\n",
    "                                  dictionary=id2word, p=1)\n",
    "    logging.info('Start building vocabulary')\n",
    "    model.build_vocab(doc_iter)\n",
    "    logging.info('Vocabulary built successfully')\n",
    "\n",
    "    for i, epoch in enumerate(range(3)):\n",
    "        logging.info('beginning interation #' + str(i) + '\\n')\n",
    "        model.train(doc_iter)\n",
    "        model.alpha -= 0.002 # decrease the learning rate\n",
    "        model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "    \n",
    "    logging.info('Persisting model')\n",
    "    model.save(join(corpus_dir, '{}_dim_wiki_doc2vec.model'.format(dim)))\n",
    "    logging.info('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "class Wordvec_Approximator:\n",
    "    def __init__(self, model=None):\n",
    "        if model == None:\n",
    "            raise TypeError('You need to pass a model')\n",
    "        self.model = model\n",
    "\n",
    "    def word_similarities(self, a):\n",
    "        '''\n",
    "        Get similarities for each word in the corpus\n",
    "        '''\n",
    "        # if vocab is None, the default vocab model.vocab is used\n",
    "        vocab = ''\n",
    "        if self.vocab == None:\n",
    "            vocab = model.vocab\n",
    "        else:\n",
    "            vocab = self.vocab\n",
    "            \n",
    "        for word in vocab.keys():\n",
    "            sim = cosine_similarity(a.reshape(1,-1), self.model[word]\n",
    "                                     .reshape(1,-1)).flatten()[0]\n",
    "            yield word, sim\n",
    "\n",
    "    def most_similar_words(self, X, Y, top=30):\n",
    "        '''\n",
    "        Find the wordvec wv that minimizes distance(X+wv, Y)\n",
    "        '''\n",
    "        diff = Y - X\n",
    "        sims = []\n",
    "        for sim in tqdm(self.word_similarities(diff)):\n",
    "            sims.append(sim)\n",
    "        sims = sorted(sims, key=lambda x: -x[1])\n",
    "        return sims[0:top]\n",
    "\n",
    "    def approximate_difference(self, X, Y, vocab=None):\n",
    "        '''\n",
    "        Compute the word2vec similarity path from X to Y\n",
    "        '''\n",
    "        # if vocab isn't passed, model.vocab will be used\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        X = X.reshape(1, -1)\n",
    "        Y = Y.reshape(1, -1)\n",
    "        newX = X\n",
    "        path = [] # keeps track of the vectors that make up the difference\n",
    "\n",
    "        logging.info('initial distance is: {}'.format(cosine_similarity(X, Y)))\n",
    "        for i in tqdm(range(20)):\n",
    "            candidates = self.most_similar_words(newX, Y)\n",
    "\n",
    "            # is the performance better now?\n",
    "            # if not, check the next candidate\n",
    "            secondChances = 10\n",
    "            newWord = \"\"\n",
    "            for j in range(0, secondChances):\n",
    "                newWord = self.model[candidates[j][0]]\n",
    "                if cosine_similarity(newX + newWord, Y) > cosine_similarity(newX, Y):\n",
    "                    logging.info('better performance with: {}'\n",
    "                                 .format(candidates[j][0]))\n",
    "                    newWord = self.model[candidates[j][0]]\n",
    "                    break\n",
    "                else:\n",
    "                    logging.info('worse performance with: {}'\n",
    "                                 .format(candidates[j][0]))\n",
    "            else:\n",
    "                logging.info('Optimization stopped after {} attempts'\n",
    "                             .format(secondChances))\n",
    "                break\n",
    "\n",
    "            logging.info('{} best word: {}'.format(i, candidates[j][0]))\n",
    "            path.append(candidates[j][0])\n",
    "            newX = newX + newWord\n",
    "            logging.info('cosine similarity: {}'.format(cosine_similarity(newX, Y)))\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-253bcac7dd80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
